# AI Storefront Tracing Implementation

This directory contains a comprehensive tracing solution for monitoring LLM calls and chat operations in the AI Storefront application.

## Overview

Our tracing implementation provides detailed observability for:
- **OpenAI API calls** with token usage, model information, and performance metrics
- **Chat operations** with thread tracking and user context
- **Error handling** with detailed error logging and status codes
- **Performance monitoring** with request/response timing

## Architecture

### Simple Tracing (`simple.ts`)

A lightweight, Convex-compatible tracing implementation that:
- Works within Convex's runtime constraints
- Generates structured JSON logs for easy ingestion
- Provides OpenTelemetry-style semantics without dependencies
- Supports span hierarchies and event logging

## Features

### LLM Call Tracing

Every OpenAI API call is automatically traced with:

```typescript
{
  "timestamp": "2025-08-16T15:27:07.073Z",
  "level": "INFO",
  "message": "TRACE: openai.chat.completions.create",
  "trace": {
    "traceId": "unique-trace-id",
    "spanId": "unique-span-id",
    "operation": "openai.chat.completions.create",
    "duration": "4283ms",
    "status": "OK"
  },
  "attributes": {
    "service.name": "ai-storefront",
    "service.version": "1.0.0",
    "llm.vendor": "openai",
    "llm.request.model": "gpt-4o-mini",
    "llm.request.temperature": 0.7,
    "llm.usage.prompt_tokens": 159,
    "llm.usage.completion_tokens": 237,
    "llm.usage.total_tokens": 396,
    "user.id": "test-user",
    "thread.id": "test-thread-123",
    "response.finish_reason": "stop"
  }
}
```

### Chat Turn Tracing

Complete chat interactions are traced with:

```typescript
{
  "message": "TRACE: chat.continue_thread",
  "trace": {
    "operation": "chat.continue_thread",
    "duration": "5586ms"
  },
  "attributes": {
    "chat.thread_id": "test-thread-456",
    "chat.user_id": "anonymous-user-id",
    "chat.turn_id": "test-thread-456-1755358043101"
  },
  "events": [
    {
      "name": "chat.turn.start",
      "timestamp": 1755358043101
    },
    {
      "name": "chat.turn.complete",
      "timestamp": 1755358048686
    }
  ]
}
```

## Usage

### Tracing LLM Calls

```typescript
import { traceLLMCall } from "./tracing/simple";

const response = await traceLLMCall(
  "openai.chat.completions.create",
  {
    model: "gpt-4o-mini",
    provider: "openai",
    temperature: 0.7,
    prompt: userPrompt,
    userId: ctx.user._id,
    threadId: args.threadId,
  },
  async (span) => {
    // Your OpenAI API call here
    const completion = await openai.chat.completions.create({...});
    
    // Update span with actual usage
    span.setAttributes({
      'llm.usage.prompt_tokens': completion.usage.prompt_tokens,
      'llm.usage.completion_tokens': completion.usage.completion_tokens,
    });
    
    return completion.choices[0]?.message?.content || "";
  }
);
```

### Tracing Chat Operations

```typescript
import { traceChatTurn } from "./tracing/simple";

return traceChatTurn(
  "continue_thread",
  {
    userId: ctx.anonymousUserId,
    threadId: args.threadId,
    turnId: `${args.threadId}-${Date.now()}`,
  },
  async () => {
    // Your chat logic here
    return await processUserMessage(args.prompt);
  }
);
```

## Integration with Observability Platforms

The structured JSON logs can be easily ingested by:

### Datadog
```bash
# Add to your log pipeline
{
  "source": "convex",
  "service": "ai-storefront",
  "trace_id": "{{ trace.traceId }}",
  "span_id": "{{ trace.spanId }}",
  "operation_name": "{{ trace.operation }}",
  "duration": "{{ trace.duration }}",
  "status": "{{ trace.status }}"
}
```

### New Relic
```javascript
// Log forwarder configuration
{
  "logtype": "ai-storefront-traces",
  "attributes": {
    "trace.id": "{{ trace.traceId }}",
    "span.id": "{{ trace.spanId }}",
    "llm.model": "{{ attributes.llm.request.model }}",
    "llm.tokens.total": "{{ attributes.llm.usage.total_tokens }}"
  }
}
```

### Custom Analytics
```sql
-- Example BigQuery schema
SELECT
  timestamp,
  JSON_EXTRACT_SCALAR(attributes, '$.llm.vendor') as llm_provider,
  JSON_EXTRACT_SCALAR(attributes, '$.llm.request.model') as model,
  CAST(JSON_EXTRACT_SCALAR(attributes, '$.llm.usage.total_tokens') AS INT64) as total_tokens,
  CAST(REGEXP_EXTRACT(trace.duration, r'(\d+)ms') AS INT64) as duration_ms
FROM ai_storefront_traces
WHERE trace.operation = 'openai.chat.completions.create'
```

## Metrics and Alerts

### Key Metrics to Monitor

1. **LLM Performance**
   - Average response time per model
   - Token usage trends
   - Error rates by provider

2. **Chat Quality**
   - Average conversation length
   - User engagement patterns
   - Fallback usage frequency

3. **Cost Optimization**
   - Token costs per conversation
   - Model usage distribution
   - Response quality vs. cost

### Sample Alerts

```yaml
# Datadog Monitor
- name: "High LLM Response Time"
  query: "avg(last_5m):avg:ai_storefront.llm.duration{*} > 10000"
  message: "LLM response times are above 10 seconds"

- name: "LLM Error Rate"
  query: "sum(last_10m):count:ai_storefront.llm.errors{*} > 5"
  message: "Multiple LLM errors detected"

- name: "High Token Usage"
  query: "sum(last_1h):sum:ai_storefront.llm.tokens{*} > 100000"
  message: "Token usage exceeding hourly budget"
```

## Environment Configuration

Set these environment variables to control tracing behavior:

```bash
# Service identification
OTEL_SERVICE_NAME=ai-storefront
OTEL_SERVICE_VERSION=1.0.0

# Development settings
NODE_ENV=development
OTEL_ENABLE_CONSOLE=true

# Production settings (future)
OTEL_ENABLE_OTLP=true
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.honeycomb.io
OTEL_EXPORTER_OTLP_HEADERS={"x-honeycomb-team":"your-api-key"}
```

## Performance Impact

The simple tracing implementation has minimal overhead:
- **Memory**: ~1KB per trace
- **CPU**: <1ms per operation
- **Network**: No network calls (logs only)
- **Storage**: JSON logs for external processing

## Future Enhancements

1. **Real-time Streaming**: WebSocket integration for live trace viewing
2. **Sampling**: Configurable sampling rates for high-volume environments  
3. **Correlation**: Link traces across microservices
4. **Metrics Export**: Direct integration with Prometheus/StatsD
5. **Custom Dashboards**: Pre-built Grafana/Datadog dashboards

## Troubleshooting

### Common Issues

1. **Missing Traces**: Check that functions import the tracing module
2. **High Volume**: Implement sampling in production environments
3. **Log Ingestion**: Ensure log parsing handles JSON format correctly

### Debug Mode

Enable verbose logging:
```typescript
// In your Convex function
console.log('ðŸ”§ Tracing Debug:', {
  hasTracer: !!tracer,
  activeSpans: tracer.activeSpans?.length || 0,
});
```

---

This tracing implementation provides production-ready observability for your AI application, enabling you to monitor performance, optimize costs, and ensure reliable service delivery.